# Copyright 2018 Google LLC.
#
# Redistribution and use in source and binary forms, with or without
# modification, are permitted provided that the following conditions
# are met:
#
# 1. Redistributions of source code must retain the above copyright notice,
#    this list of conditions and the following disclaimer.
#
# 2. Redistributions in binary form must reproduce the above copyright
#    notice, this list of conditions and the following disclaimer in the
#    documentation and/or other materials provided with the distribution.
#
# 3. Neither the name of the copyright holder nor the names of its
#    contributors may be used to endorse or promote products derived from this
#    software without specific prior written permission.
#
# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE
# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
# POSSIBILITY OF SUCH DAMAGE.

# pylint: disable=line-too-long
r"""Shuffle tf.Example files using beam.

To run locally:
1) Install beam on your machine following the instructions at
   https://beam.apache.org/get-started/quickstart-py/

2) Copy any inputs to be on local disk.

3) Run
  python path/to/shuffle_tfrecords_beam.py \
    --input_pattern_list="/tmp/some.examples-?????-of-00200.tfrecord.gz" \
    --output_pattern_prefix="/tmp/training.examples" \
    --output_dataset_name="HG001" \
    --runner=DirectRunner

To run on Google Cloud Dataflow Service:
1) Follow the Google Cloud Dataflow setup instructions at
https://beam.apache.org/documentation/runners/dataflow/

2) Upload this file to your GCE instance.

3) Run
  python shuffle_tfrecords_beam.py \
  --job_name=shuffle-tfrecords \
  --input_pattern_list="gs://YOUR_INPUT_BUCKET/A.tfrecord.gz" \
  --output_pattern_prefix="gs://YOUR_OUTPUT_BUCKET/training.examples" \
  --output_dataset_name="HG001" \
  --runner=DataflowRunner \
  --project=SET_YOUR_PROJECT_ID_HERE \
  --staging_location=gs://YOUR_BUCKET_NAME/AND_STAGING_DIRECTORY \
  --temp_location=gs://YOUR_BUCKET_NAME/AND_TEMP_DIRECTORY

4) (Optional) To monitor or cancel the job while it is running, you can
use either the Dataflow Monitoring Interface
https://cloud.google.com/dataflow/pipelines/dataflow-monitoring-intf
or the Dataflow Command-line Interface
https://cloud.google.com/dataflow/pipelines/dataflow-command-line-intf
"""
# pylint: enable=line-too-long

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import argparse
import logging
import glob

import apache_beam as beam
from apache_beam import coders
from apache_beam.options.pipeline_options import PipelineOptions
import os

COMMENT_HEADER = """# Generated by shuffle_tfrecords_beam.py
#
# --input_pattern_list={}
# --output_pattern_prefix={}
#
"""


def parse_cmdline(argv):
  """Parse the commandline into known and pipeline arguments.

  The known arguments are required for this specific program to function,
  and the other pipeline arguments can be used to configure beam and the
  specific beam backend being used.  See
  https://github.com/apache/beam/blob/master/sdks/python/apache_beam/options/pipeline_options.py
  for a list and description of the pipeline arguments accepted.

  Args:
    argv: List containing command-line arguments.

  Returns:
    A pair, the first of which are the known (non-pipeline) arguments
    and the second of which are the pipeline arguments.
  """
  parser = argparse.ArgumentParser()

  parser.add_argument(
      '--input_pattern_list',
      help='Comma-separated list of TFRecord filename patterns.')
  parser.add_argument(
    '--num_buckets', help="Number for partitioning",
    default=1, type=int,
  )
  parser.add_argument(
    "--workdir", help="Working directory",
    required=True,
  )
  parser.add_argument(
    "--debug",
    help="Enable debug messages", action="store_true", default=False,
  )

  known_args, pipeline_args = parser.parse_known_args(argv)

  assert(known_args.num_buckets > 0), \
    "Minimum of one bucket should be provided for pre-partition"

  return known_args, pipeline_args


def partition(example, num_buckets):
  """ Partition examples into multiple buckets """
  def determine_hash(input_bytes):
    import hashlib
    m = hashlib.blake2b(input_bytes, digest_size=4)
    address = 0
    for i, b in enumerate(m.digest()):
      address += b * (2 ** (i * 8))
    return address

  address = determine_hash(example)
  bucket = address % num_buckets
  return bucket


def partition_wrapper(input_pattern, working_directory, pipeline_args, num_buckets):
  """
  Wrapper for partitioning the data randomly
  """
  if os.path.exists(working_directory):
    raise ValueError("Provide a clean working directory path, so this script can create it")

  os.makedirs(working_directory)

  logging.info("Initiating partitioning")

  with beam.Pipeline(options=PipelineOptions(pipeline_args)) as pipeline:
    input_data = read_from_tfrecords_files(pipeline, input_pattern)
    partitions = input_data | "PartitionData" >> beam.Partition(partition, num_buckets)
    for i, p in enumerate(partitions):
      write = p | "WritePartition%d" % i >> beam.io.WriteToTFRecord(
        file_path_prefix=os.path.join(
          working_directory, "partition%05d" % i), file_name_suffix=".tfrecord.gz", coder=coders.BytesCoder()
        )

  filenames = glob.glob(os.path.join(working_directory, "partition?????-?????-of-?????.tfrecord.gz"))
  return filenames


def read_from_tfrecords_files(pipeline, input_filename_pattern_list):
  """Reads records from TFRecord files.

  Args:
    pipeline: Beam pipeline object.
    input_filename_pattern_list: List of filename patterns.

  Returns:
    A PCollection of read tf.Examples.
  """
  readers = []
  for i, filepattern in enumerate(input_filename_pattern_list):
    readers.append(pipeline
                   | 'ReadTFRecordFiles_{}[{}]'.format(i, filepattern) >> beam
                   .io.ReadFromTFRecord(filepattern, coder=coders.BytesCoder()))
  all_readers = readers | 'Flatten' >> beam.Flatten()
  return all_readers


def main(argv=None):
  """Main entry point; defines and runs the pipeline."""
  known_args, pipeline_args = parse_cmdline(argv)
  if known_args.debug:
    logging.getLogger().setLevel(logging.DEBUG)
  else:
    logging.getLogger().setLevel(logging.INFO)

  file_list = partition_wrapper(
    known_args.input_pattern_list.split(","),
    known_args.workdir, pipeline_args, known_args.num_buckets
  )

  logging.info("Partitioned files are in %s" % known_args.workdir)

if __name__ == '__main__':
  main()
